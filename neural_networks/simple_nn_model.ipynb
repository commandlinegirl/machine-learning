{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes of different matrices\n",
    "* Ai -- activations from a layer (layers_dims[i], num_examples)\n",
    "* Wi -- weight matrix of shape (layers_dims[i], layers_dims[i-1])\n",
    "* bi -- bias vector of shape (layers_dims[i], 1)\n",
    "* X -- input data, numpy array of shape (input size, number of examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, init_type='random'):\n",
    "    \"\"\"\n",
    "    Performs a given initialization for the weight matrices and zero initialization for biases.\n",
    "    layer_dims -- list with the dimensions of each layer in our network\n",
    "    init_type: either 'random', 'he', 'xavier'\n",
    "    Returns:\n",
    "    parameters -- dictionary with parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        if init_type == 'random':\n",
    "            scaling_factor = 0.01\n",
    "        elif init_type == 'he':\n",
    "            scaling_factor = np.sqrt(2.0 / layer_dims[l-1])\n",
    "        elif init_type == 'xavier':\n",
    "            scaling_factor = np.sqrt(1.0 / layer_dims[l-1])\n",
    "        else:\n",
    "            raise Error(\"Initialization type \" + init_type + \" not supported\")\n",
    "    \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * scaling_factor\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_initialization(layer_dims):\n",
    "    \"\"\"\n",
    "    Performs random initialization with a scaling factor 0.01\n",
    "    \"\"\"\n",
    "    return initialize_parameters(layer_dims, init_type='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_initialization(layer_dims):\n",
    "    \"\"\"\n",
    "    Performs Xavier initialization with a scaling factor sqrt(1/prev_layer_dimension)\n",
    "    \"\"\"\n",
    "    return initialize_parameters(layer_dims, init_type='xavier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def he_initialization(layer_dims):\n",
    "    \"\"\"\n",
    "    Performs He initialization with a scaling factor sqrt(2/prev_layer_dimension)\n",
    "    \"\"\"  \n",
    "    return initialize_parameters(layer_dims, init_type='he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(vec, num_examples):\n",
    "    if len(vec) == 0:\n",
    "        return vec\n",
    "    permutation = list(np.random.permutation(num_examples))\n",
    "    return vec[:, permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, shape (input size, number of examples)\n",
    "    Y -- ground thruth vector, shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "        \n",
    "    shuffled_X = shuffle(X)\n",
    "    shuffled_Y = shuffle(Y).reshape((1,m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        num_remaining_examples = m - mini_batch_size * np.floor(m / mini_batch_size)\n",
    "        last_example_index = num_complete_minibatches * mini_batch_size + num_remaining_examples\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : last_example_index]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : last_example_index]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    tmp = np.array(z, copy=True)\n",
    "    np.maximum(tmp, 0, tmp) # modifies x in place\n",
    "    return (tmp, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1.0 / (1 + np.exp(-z)), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    z = cache\n",
    "    dZ = dA * (z * (1 - dA))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    dZ, z = None, cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[ z <= 0 ] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation with a linear step and an activation step, for one layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data)\n",
    "    W -- weights matrix\n",
    "    b -- bias vector\n",
    "    activation -- the activation to be used in this layer: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function (post-activation value)\n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Linear step\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    \n",
    "    # Activation step\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    else:\n",
    "        raise Error(\"Activation function \" + activation + \" is not supported.\")\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the relu (hidden layers) and sigmoid (output layer) computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of 2-tuples storing caches output of linear_activation_forward(): [(A_prev, W, b), Z]\n",
    "              first is linear_cache, which is a 3-tuple (A_prev, W, b)\n",
    "              second is activation_cache, which is a Z array (input for the activation function)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l + 1)], parameters[\"b\"+str(l + 1)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    A_last, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(A_last.shape == (1,X.shape[1]))\n",
    "\n",
    "    return A_last, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) stored for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache # ((A_prev, W, b), Z)\n",
    "    \n",
    "    # activation step\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    else:\n",
    "        raise Error(\"Activation function \" + activation + \" is not supported.\")      \n",
    "    \n",
    "    # linear step\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    Compute gradients for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    A_last -- probability vector, output of the forward propagation\n",
    "    Y --  the ground truth vector \n",
    "    caches -- list of caches containing:\n",
    "                the caches of linear_activation_forward() with \"relu\" and \n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (last cache in the list)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients for each layer\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = A_last.shape[1]\n",
    "    Y = Y.reshape(A_last.shape) # make Y the same shape as A_last\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, A_last) - np.divide(1 - Y, 1 - A_last))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \\\n",
    "        linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\" + str(l + 1)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] = \\\n",
    "            linear_activation_backward(grads[\"dA\" + str(L)], current_cache, \"relu\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2_regularization_cost(parameters, lambd):\n",
    "    \"\"\"\n",
    "    Return L2 regularization term.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "\n",
    "    Returns:\n",
    "    cost - value of the regularization term to be used for cost function regularization\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2 # of layers in the network\n",
    "\n",
    "    sum_of_squares = 0\n",
    "    for l in range(L):\n",
    "        sum_of_squares += np.sum(np.square(parameters[\"W\"+str(l+1)]))\n",
    "    \n",
    "    L2_regularization_cost = 1/m * lambd/2 * sum_of_squares\n",
    "    \n",
    "    return L2_regularization_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    A_last -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the loss function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A_last), Y) + np.multiply(np.log(1 - A_last), 1 - Y)\n",
    "    cost = - 1.0 / m * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A_last, Y, parameters, lambd=0.01):\n",
    "    return compute_cost(A_last, Y) + get_L2_regularization_cost(parameters, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test L2 regularizartion\n",
    "\n",
    "A_last = np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]])\n",
    "Y = np.array([[1, 1, 0, 1, 0]])\n",
    "parameters = {'W1': np.array([[ 1.62434536, -0.61175641, -0.52817175],\n",
    "       [-1.07296862,  0.86540763, -2.3015387 ]]), 'b1': np.array([[ 1.74481176],\n",
    "       [-0.7612069 ]]), 'W2': np.array([[ 0.3190391 , -0.24937038],\n",
    "       [ 1.46210794, -2.06014071],\n",
    "       [-0.3224172 , -0.38405435]]), 'b2': np.array([[ 1.13376944],\n",
    "       [-1.09989127],\n",
    "       [-0.17242821]]), 'W3': np.array([[-0.87785842,  0.04221375,  0.58281521]]), 'b3': np.array([[-1.10061918]])}\n",
    "\n",
    "cost = compute_cost_with_regularization(A_last, Y, parameters, lambd = 0.1)\n",
    "assert(cost - (1.60250160476) < 10e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=1000, mini_batch_size=64,\n",
    "             seed=0, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a neural network with relu activation function for the hidden layers and\n",
    "    a sigmoid actication function for the output layer.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector, of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims, init_type='random')\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A_last, caches = forward_propagation(X, parameters)\n",
    "        cost = compute_cost_with_regularization(A_last, Y, parameters)\n",
    "        grads = backward_propagation(A_last, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print cost\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(train_x, train_y, parameters):\n",
    "    pass\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initialize params\n",
    "parameters = initialize_parameters([2, 4, 1], init_type='he')\n",
    "assert(parameters[\"W1\"][0][0] - 1.78862847 < 10e-8)\n",
    "assert(parameters[\"W1\"][0][1] - 0.43650985 < 10e-8)\n",
    "assert(parameters[\"b1\"][0][0] - 0.0 < 10e-8)\n",
    "assert(parameters[\"W2\"][0][0] + 0.00043818 < 10e-8)\n",
    "assert(parameters[\"b2\"][0][0] - 0.0 < 10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initialize params\n",
    "parameters = initialize_parameters([2, 4, 1], init_type='xavier')\n",
    "assert(parameters[\"W1\"][0][0] - 1.78862847 < 10e-8)\n",
    "assert(parameters[\"W1\"][0][1] - 0.43650985 < 10e-8)\n",
    "assert(parameters[\"b1\"][0][0] - 0.0 < 10e-8)\n",
    "assert(parameters[\"W2\"][0][0] + 0.021909084488 < 10e-8)\n",
    "assert(parameters[\"b2\"][0][0] - 0.0 < 10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initialize params\n",
    "parameters = initialize_parameters([2, 4, 1])\n",
    "assert(parameters[\"W1\"][0][0] - 1.78862847 < 10e-8)\n",
    "assert(parameters[\"W1\"][0][1] - 0.43650985 < 10e-8)\n",
    "assert(parameters[\"b1\"][0][0] - 0.0 < 10e-8)\n",
    "assert(parameters[\"W2\"][0][0] - (-0.00043818) < 10e-8)\n",
    "assert(parameters[\"b2\"][0][0] - 0.0 < 10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu([0, 2]) = (array([0, 2]), array([0, 2]))\n"
     ]
    }
   ],
   "source": [
    "# Test relu\n",
    "print (\"relu([0, 2]) = \" + str(relu(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = (array([ 0.5       ,  0.88079708]), array([0, 2]))\n",
      "exptected: [ 0.5 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test sigmoid\n",
    "print(\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "print(\"exptected: [ 0.5 0.88079708]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999996  0.99999969  1.        ]\n",
      " [ 0.99966465  0.99987661  0.9999546 ]\n",
      " [ 1.          1.          1.        ]\n",
      " [ 0.99999989  0.99999998  1.        ]]\n",
      "[[ 17.  15.  27.]\n",
      " [  8.   9.  10.]\n",
      " [ 21.  24.  27.]\n",
      " [ 16.  18.  20.]]\n"
     ]
    }
   ],
   "source": [
    "# Test linear_activation_forward\n",
    "A_prev = np.array([[ 4.,  7.,  3.], [ 2.,  0.,  5.]])\n",
    "W = np.array([[ 2.,  4.], [ 1.,  1.], [ 3.,  3.], [ 2.,  2.]])\n",
    "b = np.array([[ 1.], [ 2.], [ 3.], [ 4.]])\n",
    "\n",
    "A, _ = linear_activation_forward(A_prev, W, b, 'sigmoid')\n",
    "A_expected = np.array(\n",
    "[[ 0.99999996,  0.99999969,  1.        ],\n",
    " [ 0.99966465,  0.99987661,  0.9999546 ],\n",
    " [ 1.,          1.,          1.        ],\n",
    " [ 0.99999989,  0.99999998,  1.        ]])\n",
    "\n",
    "print(A)\n",
    "\n",
    "A, _ = linear_activation_forward(A_prev, W, b, 'relu')\n",
    "A_expected = np.array(\n",
    "[[ 17.,  15.,  27.],\n",
    " [  8.,   9.,  10.],\n",
    " [ 21.,  24.,  27.],\n",
    " [ 16.,  18.,  20.]])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caches\n",
      "[((array([[ 4.,  7.],\n",
      "       [ 2.,  0.]]), array([[ 0.01788628,  0.0043651 ]]), array([[ 0.]])), array([[ 0.08027534,  0.12520399]]))]\n",
      "A_last = [[ 0.52005806  0.53126017]]\n",
      "Length of caches list = 1\n",
      "expected A_last: [[ 0.52005806  0.53126017]]\n",
      "expected caches list len: 1\n"
     ]
    }
   ],
   "source": [
    "# Test forward_propagation\n",
    "X = np.array([[ 4.,  7.], [ 2.,  0.]])\n",
    "parameters = initialize_parameters([2, 1])\n",
    "\n",
    "A_last, caches = forward_propagation(X, parameters)\n",
    "print(\"A_last = \" + str(A_last))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n",
    "\n",
    "print(\"expected A_last: [[ 0.52005806  0.53126017]]\")\n",
    "print(\"expected caches list len: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db1': array([[-0.]]), 'dW1': array([[-0., -0.]]), 'dA1': array([[-0.00806972, -0.09111041],\n",
      "       [-0.0019694 , -0.02223526]])}\n",
      "expected:\n",
      "dW1: [[-0. -0.]]\n",
      "db1: [[-0.]]\n",
      "dA1: [[-0.00806972 -0.09111041][-0.0019694  -0.02223526]]\n"
     ]
    }
   ],
   "source": [
    "# Test backward_propagation\n",
    "Y = np.array([[1, 2]])\n",
    "A_last = np.array([[ 0.52005806,  0.53126017]])\n",
    "caches = [((np.array([[ 4.,  7.], [ 2.,  0.]]), np.array([[ 0.01788628,  0.0043651 ]]), np.array([[ 0.]])),\n",
    "          np.array([[ 0.08027534,  0.12520399]]))]\n",
    "\n",
    "grads =  backward_propagation(A_last, Y, caches)\n",
    "\n",
    "print(grads)\n",
    "\n",
    "print(\"expected:\")\n",
    "print(\"dW1: [[-0. -0.]]\")\n",
    "print(\"db1: [[-0.]]\")\n",
    "print(\"dA1: [[-0.00806972 -0.09111041][-0.0019694  -0.02223526]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test compute_cost\n",
    "A_last = np.array([[0.40682402, 0.01629284, 0.16722898, 0.10118111, 0.40682402]])\n",
    "Y = np.array([[1, 1, 0, 1, 0]])\n",
    "\n",
    "assert(compute_cost(A_last, Y) - 1.60250160476 < 10e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
