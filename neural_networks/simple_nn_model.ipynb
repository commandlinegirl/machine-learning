{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of shapes:\n",
    "* Ai -- activations from a layer (layers_dims[i], num_examples)\n",
    "* Wi -- weight matrix of shape (layers_dims[i], layers_dims[i-1])\n",
    "* bi -- bias vector of shape (layers_dims[i], 1)\n",
    "* X -- input data, numpy array of shape (input size, number of examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, init_type='random'):\n",
    "    \"\"\"\n",
    "    Performs a given initialization for the weight matrices and zero initialization for biases.\n",
    "    layer_dims -- list with the dimensions of each layer in our network\n",
    "    init_type: either 'random', 'he', 'xavier'\n",
    "    Returns:\n",
    "    parameters -- dictionary with parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        if init_type == 'random':\n",
    "            scaling_factor = 0.01\n",
    "        elif init_type == 'he':\n",
    "            scaling_factor = np.sqrt(2.0 / layer_dims[l-1])\n",
    "        elif init_type == 'xavier':\n",
    "            scaling_factor = np.sqrt(1.0 / layer_dims[l-1])\n",
    "        else:\n",
    "            raise Error(\"Initialization type \" + init_type + \" not supported\")\n",
    "    \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * scaling_factor\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_initialization(layer_dims):\n",
    "    \"\"\"\n",
    "    Performs random initialization with a scaling factor 0.01\n",
    "    \"\"\"\n",
    "    return initialize_parameters(layer_dims, init_type='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_initialization(layer_dims):\n",
    "    \"\"\"\n",
    "    Performs Xavier initialization with a scaling factor sqrt(1/prev_layer_dimension)\n",
    "    \"\"\"\n",
    "    return initialize_parameters(layer_dims, init_type='xavier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def he_initialization(layer_dims):\n",
    "    \"\"\"\n",
    "    Performs He initialization with a scaling factor sqrt(2/prev_layer_dimension)\n",
    "    \"\"\"  \n",
    "    return initialize_parameters(layer_dims, init_type='he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([2, 4, 1], init_type='he')\n",
    "assert(parameters[\"W1\"][0][0] - 1.78862847 < 10e-8)\n",
    "assert(parameters[\"W1\"][0][1] - 0.43650985 < 10e-8)\n",
    "assert(parameters[\"b1\"][0][0] - 0.0 < 10e-8)\n",
    "assert(parameters[\"W2\"][0][0] + 0.00043818 < 10e-8)\n",
    "assert(parameters[\"b2\"][0][0] - 0.0 < 10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([2, 4, 1], init_type='xavier')\n",
    "assert(parameters[\"W1\"][0][0] - 1.78862847 < 10e-8)\n",
    "assert(parameters[\"W1\"][0][1] - 0.43650985 < 10e-8)\n",
    "assert(parameters[\"b1\"][0][0] - 0.0 < 10e-8)\n",
    "assert(parameters[\"W2\"][0][0] + 0.021909084488 < 10e-8)\n",
    "assert(parameters[\"b2\"][0][0] - 0.0 < 10e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([2, 4, 1])\n",
    "assert(parameters[\"W1\"][0][0] - 1.78862847 < 10e-8)\n",
    "assert(parameters[\"W1\"][0][1] - 0.43650985 < 10e-8)\n",
    "assert(parameters[\"b1\"][0][0] - 0.0 < 10e-8)\n",
    "assert(parameters[\"W2\"][0][0] - (-0.00043818) < 10e-8)\n",
    "assert(parameters[\"b2\"][0][0] - 0.0 < 10e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_L2_regularization_cost(parameters, lambd):\n",
    "    \"\"\"\n",
    "    Return L2 regularization term.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "\n",
    "    Returns:\n",
    "    cost - value of the regularization term to be used for cost function regularization\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "\n",
    "    sum_of_squares = np.sum(np.square(W1) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "    L2_regularization_cost = 1/m * lambd/2 * sum_of_squares\n",
    "    \n",
    "    return L2_regularization_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(vec, num_examples):\n",
    "    if len(vec) == 0:\n",
    "        return vec\n",
    "    permutation = list(np.random.permutation(num_examples))\n",
    "    return vec[:, permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, shape (input size, number of examples)\n",
    "    Y -- ground thruth vector, shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "        \n",
    "    shuffled_X = shuffle(X)\n",
    "    shuffled_Y = shuffle(Y).reshape((1,m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        num_remaining_examples = m - mini_batch_size * np.floor(m / mini_batch_size)\n",
    "        last_example_index = num_complete_minibatches * mini_batch_size + num_remaining_examples\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : last_example_index]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : last_example_index]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    x = np.array(z, copy=True)\n",
    "    np.maximum(x, 0, x) # modifies x in place\n",
    "    return (x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu([0, 2]) = (array([0, 2]), array([0, 2]))\n"
     ]
    }
   ],
   "source": [
    "# Test relu\n",
    "print (\"relu([0, 2]) = \" + str(relu(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return  (1.0 / (1 + np.exp(-z)), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = (array([ 0.5       ,  0.88079708]), array([0, 2]))\n",
      "exptected: [ 0.5 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test sigmoid\n",
    "print(\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "print(\"exptected: [ 0.5 0.88079708]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation with a linear step and an activation step, for one layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data)\n",
    "    W -- weights matrix\n",
    "    b -- bias vector\n",
    "    activation -- the activation to be used in this layer: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function (post-activation value)\n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Linear step\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    \n",
    "    # Activation step\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    else:\n",
    "        raise Error(\"Activation function \" + activation + \" is not supported.\")\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999996  0.99999969  1.        ]\n",
      " [ 0.99966465  0.99987661  0.9999546 ]\n",
      " [ 1.          1.          1.        ]\n",
      " [ 0.99999989  0.99999998  1.        ]]\n",
      "[[ 17.  15.  27.]\n",
      " [  8.   9.  10.]\n",
      " [ 21.  24.  27.]\n",
      " [ 16.  18.  20.]]\n"
     ]
    }
   ],
   "source": [
    "# Test linear_activation_forward\n",
    "A_prev = np.array([[ 4.,  7.,  3.], [ 2.,  0.,  5.]])\n",
    "W = np.array([[ 2.,  4.], [ 1.,  1.], [ 3.,  3.], [ 2.,  2.]])\n",
    "b = np.array([[ 1.], [ 2.], [ 3.], [ 4.]])\n",
    "\n",
    "A, _ = linear_activation_forward(A_prev, W, b, 'sigmoid')\n",
    "A_expected = np.array(\n",
    "[[ 0.99999996,  0.99999969,  1.        ],\n",
    " [ 0.99966465,  0.99987661,  0.9999546 ],\n",
    " [ 1.,          1.,          1.        ],\n",
    " [ 0.99999989,  0.99999998,  1.        ]])\n",
    "\n",
    "print(A)\n",
    "\n",
    "A, _ = linear_activation_forward(A_prev, W, b, 'relu')\n",
    "A_expected = np.array(\n",
    "[[ 17.,  15.,  27.],\n",
    " [  8.,   9.,  10.],\n",
    " [ 21.,  24.,  27.],\n",
    " [ 16.,  18.,  20.]])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the relu (hidden layers) and sigmoid (output layer) computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches output of linear_activation_forward() \n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l + 1)], parameters[\"b\"+str(l + 1)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    A_last, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(A_last.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return A_last, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_last = [[ 0.52005806  0.53126017]]\n",
      "Length of caches list = 1\n",
      "expected A_last: [[ 0.52005806  0.53126017]]\n",
      "expected caches list len: 1\n"
     ]
    }
   ],
   "source": [
    "# Test forward_propagation\n",
    "X = np.array([[ 4.,  7.], [ 2.,  0.]])\n",
    "parameters = initialize_parameters([2, 1])\n",
    "\n",
    "A_last, caches = forward_propagation(X, parameters)\n",
    "print(\"A_last = \" + str(A_last))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n",
    "\n",
    "print(\"expected A_last: [[ 0.52005806  0.53126017]]\")\n",
    "print(\"expected caches list len: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    else:\n",
    "        raise Error(\"Activation function \" + activation + \" is not supported.\")      \n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \\\n",
    "        linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        j = str(l + 1)\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\" + j], grads[\"dW\" + j], grads[\"db\" + j] = \\\n",
    "            linear_activation_backward(grads[\"dA\" + str(L)], current_cache, \"relu\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A_last -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the loss function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A_last), Y) + np.multiply(np.log(1 - A_last), 1 - Y)\n",
    "    cost = - 1.0 / m * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test compute_cost()\n",
    "A_last = np.array([[0.40682402, 0.01629284, 0.16722898, 0.10118111, 0.40682402]])\n",
    "Y = np.array([[1, 1, 0, 1, 0]])\n",
    "\n",
    "assert(compute_cost(A_last, Y) - 1.60250160476 < 10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, layers_dims, learning_rate=0.0075, num_iterations=1000, mini_batch_size=64,\n",
    "             seed=0, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a neural network with relu activation function for the hidden layers and\n",
    "    a sigmoid actication function for the output layer.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims, init_type='random')\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A_last, caches = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A_last, Y)\n",
    "        grads = backward_propagation(A_last, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print cost\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(train_x, train_y, parameters):\n",
    "    pass\n",
    "    return Y_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
